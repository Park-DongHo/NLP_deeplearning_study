{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "640705451e00d7610dd8e88f8f9e548784fc1bb94bf5ac5fcf7324ff83b28e30"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 1. Binary classification : 이진분류\n",
    "Encoding 0과 1로. Linear regression 으로 진행시 실제론 1인데 0으로 나오는 문제가 생길 수 있음.  \n",
    "ex) decision boundary 가 직선으로는 표현되지 않는 상황 발생.\n",
    "\n",
    "$H(x) = Wx + b$ libear hypothesis 에다 어떤 함수를 취하면 가능할 거 같은데?\n",
    "-> $H(x) = G(Wx + b) \\quad where . G(z)=1 / (1+e^-z) => sigmoid(logistic) function$\n",
    "\n",
    "결과적으로 Hypothesis는 이렇게 변형 되겠네?  \n",
    "$H(x) = 1 / (1+e^-WX)$\n",
    "\n",
    "그럼 cost fuction은 ? MSE를 그대로 사용할 수 있을까?  \n",
    "Local minim gradient 에 갇히는 문제 발생.\n",
    "\n",
    "그걸 어떻게 해결해? ‘Cross Entropy’ 가 있다!\n",
    "두 확률 분포가 얼마나 다른지를 표현. 많이 다르면 불안정하니까 엔트로피 값이 크고 비슷하면 엔트로피 값이 작아진다.  \n",
    "\n",
    "$H(P,Q) = -\\sum P(x)log(Q(x))$\n",
    "$where.$ $P(x)$실제 확률 $Q(x)$ 예측확률  \n",
    "ex) 독일을 이길 확률 예측 값: 0.01  근데 실제로 이김 그럼 P(x)=1 이됨  \n",
    "차이가 엄청 크다. 그럴 경우 엔트로피 값이 커짐\n",
    "\n",
    "즉, 값을 유사하게 예측할 수록 엔트로피 값이 낮아지므로 cost Function 으로 이용하기 적합.\n",
    "\n",
    "정의해보자\n",
    "\n",
    "$cost(W) = \\frac{1}{m} \\sum^{n}_{i=1} c(H(x^i),y^i)$  \n",
    "=> $c(H(x),y)=\\begin{cases}-log(H(x)), & \\mbox{if }y=1\\\\ -log(1-H(x)), & \\mbox{if }y=0\\end{cases}$\n",
    "\n",
    "\n",
    "이걸 한줄로 정리하면\n",
    "=> $c(H(x),y) = -ylog(H(x)) - (1-y)log(1-H(x))$\n",
    "\n",
    "### 2. Multinomial classification\n",
    "\n",
    "Multinomial classification 문제에서도 당연히\n",
    "$H(x) = Wx+b$ 로 직선을 그리는 것으론 분류할 수가 없다.\n",
    "\n",
    "그럼 어떤 가설을 다시 세우는게 나을까?  \n",
    "각각의 값들이 확률로 나오면 좋을 것 같다. 그러면 어떻게 하면 좋을까.  \n",
    "그래서 나온 것이 'softmax function'\n",
    "\n",
    "$softmax(\\hat{y_i}) = \\frac{e^{\\hat{y_i}}}{\\sum_je^{\\hat{y_j}}}$\n",
    "\n",
    "각각의 계산값들이 softmax function을 통과하면서 0과 1사이의 값을 가지며, 그 값들은 모두 합치면 1이 된다.\n",
    "\n",
    "Cost function 도 cross entropy를 그대로 가져와서 사용하지만,\n",
    "Binary 때와는 다르게 값들을 시그마를 통해 다 합쳐쥼.\n",
    "\n",
    "\n",
    "### 실습전에 data, train set / validation set / test set 에 대해\n",
    "1. data challenge를 하는 경우 train set 에서 분할하여 validation set을 만든다.(dev set)\n",
    "2. train set으로 학습을 진행하고, validation set으로 훈련을 언제 멈춰야할지를 정한다.  \n",
    "ex) train loss 가 계속 떨어질 때 over fitting을 방지하기 위해 validation loss를 보면서 둘의 차이가 커지는 시점까지 학습을 시킬 수 있다.\n",
    "\n",
    "### logistic model로 multi classification 문제를 해결 힘듬.\n",
    "decision boundary가 직선으로 나오기 때문.\n",
    "\n",
    "### logistic model에서 softmax를 사용하면 안되는 이유\n",
    "cost function으로 nn.cross_entropy를 사용하는데, 내부 로직이 softmax를 취한다. 그렇기에 저기 넣기전에 softmax 취해서 넣으면 두번 거치기 때문에 학습이 잘 안될 수 있다.\n",
    "\n",
    "즉, 함수를 쓸 때마다 document를 꼭 찾아서 확인해보자.\n",
    "\n",
    "### 좀 더 알아볼 것\n",
    "시그모이드를 어떻게 활용하게되는지에 대해"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## MLP\n",
    "\n",
    "뉴런이 활성함수를 거쳐서 특정 값을 초과하지 않으면 신호를 보내지 않는다. 활성함수는 그러다보니 non linear 한 특징을 가진다.\n",
    "\n",
    "## backpropagation with chain rule(MLP)\n",
    "1. feed forward (순전파) : computation graph를 따라가서 연산을 진행함.\n",
    "2. backpropagation : $\\frac{\\partial L}{\\partial w}$ => w가 loss에 어떤영향을끼치는가\n",
    "\n",
    "끝에서부터 부분끼리 편미분을 해나감."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}